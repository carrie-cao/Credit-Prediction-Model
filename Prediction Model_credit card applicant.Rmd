---
title: "predictive model for credit card applicants"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
the GermanCredit data set germancredit.txt from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german / (description at http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29 ),
use logistic regression to find a good predictive model for whether credit applicants are good credit risks or not.

```{r}
# Clear environment
rm(list=ls())
# Read the data 
data<-read.table("germancredit.txt",sep = " ")
head(data)

```

 Since binomial family of glm recognises 0 and 1 as the classfication values, 
 convert 1s and 2s to 0s and 1s for the response variable
```{r}
data$V21[data$V21==1]<-0
data$V21[data$V21==2]<-1
```
 
 set seed
```{r}
set.seed(1)
```
 
 Split data
```{r}

m <- nrow(data)
trn <- sample(1:m, size = round(m*0.7), replace = FALSE)
d.learn <- data[trn,]
d.valid <- data[-trn,]
```

 1st iteration: Use all the available variables

```{r}
reg = glm(V21 ~.,family=binomial(link = "logit"),data=d.learn)
summary(reg)
```

2nd iteration: Use all the variables found significant in the 1st iteration.
```{r}

reg = glm(V21 ~ V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V12+V14+V16+V20,family=binomial(link = "logit"),data=d.learn)
summary(reg)
```

 3rd iteration: Use only the significant variables obtained in the 2nd iteration.

```{r}
reg = glm(V21 ~ V1+V2+V3+V4+V5+V6+V8+V9+V10+V14+V20,family=binomial(link = "logit"),data=d.learn)
summary(reg)
```

 can create a binary variable for each significant factor:
```{r}

d.learn$V1A13[d.learn$V1 == "A13"] <- 1
d.learn$V1A13[d.learn$V1 != "A13"] <- 0

d.learn$V1A14[d.learn$V1 == "A14"] <- 1
d.learn$V1A14[d.learn$V1 != "A14"] <- 0

d.learn$V3A32[d.learn$V3 == "A32"] <- 1
d.learn$V3A32[d.learn$V3 != "A32"] <- 0

d.learn$V3A33[d.learn$V3 == "A33"] <- 1
d.learn$V3A33[d.learn$V3 != "A33"] <- 0

d.learn$V3A34[d.learn$V3 == "A34"] <- 1
d.learn$V3A34[d.learn$V3 != "A34"] <- 0

d.learn$V4A41[d.learn$V4 == "A41"] <- 1
d.learn$V4A41[d.learn$V4 != "A41"] <- 0

d.learn$V4A410[d.learn$V4 == "A410"] <- 1
d.learn$V4A410[d.learn$V4 != "A410"] <- 0

d.learn$V4A42[d.learn$V4 == "A42"] <- 1
d.learn$V4A42[d.learn$V4 != "A42"] <- 0

d.learn$V4A43[d.learn$V4 == "A43"] <- 1
d.learn$V4A43[d.learn$V4 != "A43"] <- 0

d.learn$V4A48[d.learn$V4 == "A48"] <- 1
d.learn$V4A48[d.learn$V4 != "A48"] <- 0

d.learn$V4A49[d.learn$V4 == "A49"] <- 1
d.learn$V4A49[d.learn$V4 != "A49"] <- 0

d.learn$V6A63[d.learn$V6 == "A63"] <- 1
d.learn$V6A63[d.learn$V6 != "A63"] <- 0

d.learn$V6A65[d.learn$V6 == "A65"] <- 1
d.learn$V6A65[d.learn$V6 != "A65"] <- 0

d.learn$V9A93[d.learn$V9 == "A93"] <- 1
d.learn$V9A93[d.learn$V9 != "A93"] <- 0

d.learn$V10A103[d.learn$V10 == "A103"] <- 1
d.learn$V10A103[d.learn$V10 != "A103"] <- 0

d.learn$V14A143[d.learn$V14 == "A143"] <- 1
d.learn$V14A143[d.learn$V14 != "A143"] <- 0

d.learn$V20A202[d.learn$V20 == "A202"] <- 1
d.learn$V20A202[d.learn$V20 != "A202"] <- 0
```
 
Next round model:

```{r}
reg = glm(V21 ~ V1A13 + V1A14 + V2 + V3A32 + V3A33 + V3A34 + V4A41 + V4A410 + V4A42 + V4A43 + V4A48 + V4A49 + V5 + V6A63 + V6A65 + V8 + V9A93 + V10A103 + V14A143 + V20A202,family=binomial(link = "logit"),data=d.learn)
summary(reg)
```


Remove V4A48 and V6A63 (p-value above 0.05) and V20A202 (p-value above 0.1)

```{r}
reg = glm(V21 ~ V1A13 + V1A14 + V2 + V3A32 + V3A33 + V3A34 + V4A41 + V4A410 + V4A42 + V4A43 + V4A49 + V5 + V6A65 + V8 + V9A93 + V10A103 + V14A143,family=binomial(link = "logit"),data=d.learn)
summary(reg)
```
# Validation 

add the binary variables to the validation set

```{r}
d.valid$V1A13[d.valid$V1 == "A13"] <- 1
d.valid$V1A13[d.valid$V1 != "A13"] <- 0

d.valid$V1A14[d.valid$V1 == "A14"] <- 1
d.valid$V1A14[d.valid$V1 != "A14"] <- 0

d.valid$V3A32[d.valid$V3 == "A32"] <- 1
d.valid$V3A32[d.valid$V3 != "A32"] <- 0

d.valid$V3A33[d.valid$V3 == "A33"] <- 1
d.valid$V3A33[d.valid$V3 != "A33"] <- 0

d.valid$V3A34[d.valid$V3 == "A34"] <- 1
d.valid$V3A34[d.valid$V3 != "A34"] <- 0

d.valid$V4A41[d.valid$V4 == "A41"] <- 1
d.valid$V4A41[d.valid$V4 != "A41"] <- 0

d.valid$V4A410[d.valid$V4 == "A410"] <- 1
d.valid$V4A410[d.valid$V4 != "A410"] <- 0

d.valid$V4A42[d.valid$V4 == "A42"] <- 1
d.valid$V4A42[d.valid$V4 != "A42"] <- 0

d.valid$V4A43[d.valid$V4 == "A43"] <- 1
d.valid$V4A43[d.valid$V4 != "A43"] <- 0

d.valid$V4A49[d.valid$V4 == "A49"] <- 1
d.valid$V4A49[d.valid$V4 != "A49"] <- 0

d.valid$V6A65[d.valid$V6 == "A65"] <- 1
d.valid$V6A65[d.valid$V6 != "A65"] <- 0

d.valid$V9A93[d.valid$V9 == "A93"] <- 1
d.valid$V9A93[d.valid$V9 != "A93"] <- 0

d.valid$V10A103[d.valid$V10 == "A103"] <- 1
d.valid$V10A103[d.valid$V10 != "A103"] <- 0

d.valid$V14A143[d.valid$V14 == "A143"] <- 1
d.valid$V14A143[d.valid$V14 != "A143"] <- 0

```

test the model

```{r}
y_hat<-predict(reg,d.valid,type = "response")
y_hat # y_hat is a vector of fractions.

```
use a threshold to make yes/no decisions,and view the confusion matrix.

```{r}
y_hat_round <- as.integer(y_hat > 0.5)

t <- table(y_hat_round,d.valid$V21)
t
```

Model's accuracy is (183 + 43) / (183 + 43 + 22 + 52) = 75%.

```{r}
acc <- (t[1,1] + t[2,2]) / sum(t)
acc
```

ROC curve
Develop ROC curve to determine the quality of fit

```{r}
library(pROC)
r<-roc(d.valid$V21,y_hat_round)
plot(r,main="ROC curve")
r
```


The area under the curve is 67%. This means that whenever a sample is chosen from the response group and another sample is chosen from the non-response group, then the model will correctly classify both the samples 67% of the times.

```{r}
# try more
acc <- c()
auc <- c()
for (i in 1:9) {
  y_hat_round <- as.integer(y_hat > i/10)
  t <- table(y_hat_round,d.valid$V21)
  acc <- cbind(acc,(t[1,1] + t[2,2]) / sum(t))
  r<-roc(d.valid$V21,y_hat_round)
  auc <- cbind(auc,r$auc)
}
acc
auc
```

So if we're just looking for the highest accuracy, a threshold of 0.5 looks good.
If we're judging by AUC, a smaller threshold (0.2 or 0.3) is slightly better. but not by much.

The loss of incorrectly classfying a "bad" customer is 5 times the loss of incorrectly classifying a "good" customer. calulating loss for the value of thresholds ranging from 0.01 to 1. 
```{r}
loss <- c()
for(i in 1:100)
{
  y_hat_round <- as.integer(y_hat > (i/100)) # calculate threshold predictions

  tm <-as.matrix(table(y_hat_round,d.valid$V21))

  if(nrow(tm)>1) { c1 <- tm[2,1] } else { c1 <- 0 }
  if(ncol(tm)>1) { c2 <- tm[1,2] } else { c2 <- 0 }
  loss <- c(loss, c2*5 + c1)
}

plot(c(1:100)/100,loss,xlab = "Threshold",ylab = "Loss",main = "Loss vs Threshold")
which.min(loss)

loss
```
The threshold probability corresponding to minimum expected loss is 0.13.  
The range from 0.07-0.14 is all pretty good.
The expected loss at 0.13 is 165 over the 300 validation data points.
That compares to 282 for a threshold of 0.5.So accounting for the situation is important.


```{r}
#Here's the accuracy and area-under-curve for the 0.13 threshold:
y_hat_round <- as.integer(y_hat > (which.min(loss)/100)) # find 0/1 predictions
t <- table(y_hat_round,d.valid$V21) # put in table form 
acc <- (t[1,1] + t[2,2]) / sum(t) # calculate accuracy
r<-roc(d.valid$V21,y_hat_round) # calculate ROC curve
auc <- r$auc # get AUC
acc
auc
```

